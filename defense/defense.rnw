\documentclass{beamer}                                                    
\usepackage{amssymb}                                                      
%\usepackage{pgffor}
%\usepackage{subcaption}
\usepackage{Sweave}                                                       
\usepackage{bm}                                                           
\usepackage{mathtools}                                                    
\usepackage{graphicx}
\usepackage{float}
\usepackage{amsmath} % multline
\usepackage[UKenglish]{isodate} % for: \today                             
\cleanlookdateon                % for: \today                             
                                                                          
\def\wl{\par \vspace{\baselineskip}\noindent}                             
%\def\beginmyfig{\begin{figure}[!Htbp]\begin{center}}                     
\def\beginmyfig{\begin{figure}[H]\begin{center}}                          
\def\endmyfig{\end{center}\end{figure}}                                   
\def\prodl#1#2#3{\prod\limits_{#1=#2}^{#3}}                               
\def\suml#1#2#3{\sum\limits_{#1=#2}^{#3}}                                 
\def\ds{\displaystyle}                                                    
\def\hik{\ds\frac{\suml{j}{1}{i-1}\lambda(\sigma_j,\sigma_i)z_{j,k}}
        {\suml{k}{1}{y_i}\suml{j}{1}{i-1}\lambda(\sigma_j,\sigma_i)z_{j,k}}}

% Beamer Stuff:
\usepackage{beamerthemeHannover} % Determines the Theme
\usecolortheme{seahorse}         % Determines the Color

% my title:                                                               
\title{Extending The Indian Buffet Process to Include Pairwise Distance Information}
\author[Arthur Lui]{Arthur Lui}
\institute[Brigham Young University]{
  Department of Statistics\\
  Brigham Young University
}
%\setkeys{Gin}{width=0.5\textwidth}                                       
\begin{document}                                                          
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

<<design,echo=F>>=
  source("../ddibp/R/simulation/ddibp.R",chdir=T)
@
% 8 minutes
\frame{\titlepage}
\section{Introduction}
  \frame{
    \frametitle{Introduction}
    %\begin{itemize}
    %  \item One challenge in latent variable modelling is predetermining
    %        the number of latent variables generating the observations
    %        % a type of latent variable model.
    %        % Factor Analysis: Quant, Quant
    %        % Latent Feature Model: Quant Obs, Cat Latent.
    %        \wl
    %  \item One type of latent variable model is the latent \textit{feature}
    %        model, which consists of quantitative observations and binary latent
    %        variables
    %        \wl
    %  \item The Indian Buffet Process offers a distribution for sparse
    %        infinite binary matrices, which can serve as a prior distribution
    %        for feature matrices in latent feature models
    %\end{itemize}  
  }
      
\section{IBP}

  \frame{
    \frametitle{Indian Buffet Process (IBP)}
    In the IBP, a customer $i$ taking a dish $k$ is analogous to 
    observation $i$ possessing feautre $k$. This is indicated by setting 
    $z_{ik}$ to 1 if the customer takes the dish, and 0 otherwise.\\
    \wl\noindent
    An IBP($\alpha$) for $N$ observations can be simulate as follows:
    \begin{enumerate}
      \item The first customer takes Poisson($\alpha$) number of dishes
      \item For customers $i=2 \text{ to } N$,
      \begin{itemize}
        \item For each previously sampled dish,
              customer $i$ takes dish $k$ with probability $m_k / i$
        \item after sampling the last sampled dish, customer $i$ samples 
              Poisson($\alpha/i$) new dishes
      \end{itemize}
    \end{enumerate}
  }

  \frame{
    \frametitle{PMF for the IBP}
      \[
      \begin{array}{rcl}
        \underset{N \times \infty}{\bm Z} &\sim& \text{IBP}(\alpha),
           \text{ where $z_{ik} \in \{0,1\}$} \\
        \implies \text{P}(\bm Z) & = & \frac{\alpha^{K}}{\prodl{i}{1}{N} 
                                       x_i!} 
                                       exp\{-\alpha H_N\}\prodl{k}{1}{K}
                                       \frac{(N-m_k)!(m_k-1)!}{N!},

      \end{array}  
      \]
      where $H_N$ is the harmonic number, $\suml{i}{1}{N}i^{-1}$, $K$
      is the number of non-zero columns in $\bm Z$, $m_k$ is the $k^{th}$
      column sum of $\bm Z$, and $x_i$ is the ``number of new dishes"
      sampled by customer $i$.\\
  }

  \frame{
    \frametitle{Realizations from the IBP}
    \beginmyfig
<<fig=T,echo=F,height=5>>=
    set.seed(1)
    par(mfrow=c(1,2))
      a.image(raibp(N=50,a=2),main="Figure 1: IBP(N=50, a=2)",cex.main=.8,axis.num=F)
      a.image(raibp(N=50,a=10),main="Figure 2: IBP(N=50, a=10)",cex.main=.8,axis.num=F)
    par(mfrow=c(1,1))
@
    \endmyfig
  }

\section{AIBP}  
  \frame{
    \frametitle{Attraction Indian Buffet Process (AIBP)}
    To obtain a realization $\bm Z$ from an AIBP($\alpha$) given a distance matrix $D$,
    which containts the distance information between each observation,

    \begin{enumerate}
      \item The first customer takes Poisson($\alpha$) number of dishes
      \item For customers $i=2 \text{ to } N$,
      \begin{itemize}
        \item For each previously sampled dish,
              customer $i$ takes dish $k$ with probability $p_{i,k}$
        \item after sampling the last sampled dish, customer $i$ samples 
              Poisson($\alpha/i$) new dishes
      \end{itemize}
    \end{enumerate}
    where $p_{i,k} = \hik\frac{m_{-i}}{i}$, and $m_{-i}$ is the total number of 
    dishes taken by each customer before customer $i$.
  }
  
  \frame{
    \frametitle{PMF for the AIBP}
    \begin{multline*}
      P(Z|D,\alpha) =
      \\\\
      \ds\frac{\alpha^K \exp(-\alpha H_N)} {\prodl{i}{1}{N}x_i!} 
      \prodl{i}{2}{N} i^{-(x_i+y_{-i})} \times
      \\
      \prodl{k}{1}{y_{-i}} 
      \ds\frac{
      %\left(\sums{\sigma}{z_{-i,k}=1}{}\lambda(\sigma,\sigma_i)m_{-i}\right)^{z_{i,k}}
      %\left(i-\sums{\sigma}{z_{-i,k}=1}{}
      %\lambda(\sigma,\sigma_i)m_{-i}\right)^{1-z_{i,k}}
      \left(\suml{j}{1}{i-1}\lambda(\sigma_j,\sigma_i)z_{j,k}m_{-i}\right)^{z_{i,k}}
      \left(i-\suml{j}{1}{i-1}
      \lambda(\sigma_j,\sigma_i)z_{j,k}m_{-i}\right)^{1-z_{i,k}}
      }
      %{\suml{k}{1}{y_{-i}}\sums{\sigma}{z_{-i,k}=1}{}\lambda(\sigma,\sigma_i)}\right)
      {\suml{k}{1}{y_{-i}}\suml{j}{1}{i-1}\lambda(\sigma_j,\sigma_i)z_{j,k}}
    \end{multline*}
   }

\end{document}
