\documentclass{beamer}                                                    
\usepackage{amssymb}                                                      
%\usepackage{pgffor}
%\usepackage{subcaption}
\usepackage{Sweave}                                                       
\usepackage{bm}                                                           
\usepackage{mathtools}                                                    
\usepackage{graphicx}
\usepackage{float}
\usepackage{amsmath} % multline
\usepackage{caption} %captionof
\usepackage[UKenglish]{isodate} % for: \today                             
\cleanlookdateon                % for: \today                             
                                                                          
\def\wl{\par \vspace{\baselineskip}\noindent}                             
%\def\beginmyfig{\begin{figure}[!Htbp]\begin{center}}                     
\def\beginmyfig{\begin{figure}[H]\begin{center}}                          
\def\endmyfig{\end{center}\end{figure}}                                   
\def\prodl#1#2#3{\prod\limits_{#1=#2}^{#3}}                               
\def\suml#1#2#3{\sum\limits_{#1=#2}^{#3}}                                 
\def\ds{\displaystyle}                                                    
%\def\hik{\ds\frac{\suml{j}{1}{i-1}\lambda(\sigma_j,\sigma_i)z_{j,k}}
%        {\suml{k}{1}{y_i}\suml{j}{1}{i-1}\lambda(\sigma_j,\sigma_i)z_{j,k}}}
\def\hik{\frac{\suml{j}{1}{i-1}\lambda(j,i)z_{j,k}}
        {\suml{k}{1}{y_i}\suml{j}{1}{i-1}\lambda(j,i)z_{j,k}}}

% Beamer Stuff:
\usepackage{beamerthemeHannover} % Determines the Theme
\usecolortheme{seahorse}         % Determines the Color

% my title:                                                               
\title{Extending The Indian Buffet Process to Include Pairwise Distance Information}
%\logo{\includegraphics[width=1cm,height=1cm,keepspectration]{/data/arthurll/Pictures/logo.png}}
\author[Arthur Lui]{Arthur Lui}
\institute[Brigham Young University]{
  Department of Statistics\\
  Brigham Young University
}
%\setkeys{Gin}{width=0.5\textwidth}                                       
\begin{document}                                                          
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

<<design,echo=F>>=
  source("../ddibp/R/simulation/ddibp.R",chdir=T)
@
% 8 minutes
\frame{\titlepage}
\section{Introduction}
  \frame{
    \frametitle{Latent Variable Models}
    \begin{itemize}
      \item[] Factor Analysis 
      \begin{itemize}
          \item[]
          \item $\bm{y = \Lambda f + \epsilon}$, where $\bm\Lambda$ is a
                matrix with continuous values, and $\bm f$ is a vector with
                continuous values
          \item[]
          \item[]
          \item Latent Feature Model
          \begin{itemize}
            \item $\bm{y = Zf + \epsilon}$, where $\bm Z$ is a matrix
                  with binary values
            \item The Indian buffet process is a suitable prior for 
                  modeling $\bm Z$
          \end{itemize}
          \item[]
          \item Only $\bm y$ is observed
      \end{itemize}

      % Presentation Notes:
      %\item One challenge in latent variable modelling is predetermining
      %      the number of latent variables generating the observations
      %      % a type of latent variable model.
      %      % Factor Analysis: Quant, Quant
      %      % Latent Feature Model: Quant Obs, Cat Latent.
      %      \wl
      %\item One type of latent variable model is the latent \textit{feature}
      %      model, which consists of quantitative observations and binary latent
      %      variables
      %      \wl
      %\item The Indian Buffet Process is a distribution for infinite and 
      %      sparse binary matrices, which can serve as a prior distribution
      %      for feature matrices in latent feature models
    \end{itemize}  
  }

  \frame{
    \frametitle{Research Goals}
    \begin{itemize}
      \item Propose a distribution that extends the IBP to include distance
            information
        \begin{itemize}
          \item[]
          \item The distance dependent IBP (ddIBP) constructed by Gershman, et al.,
                (2012) has only an implicit pmf
          \item[]
          \item We will propose a distribution with an explicit pmf      
        \end{itemize}
    \end{itemize}
  }

\section{IBP}
  \frame{
    \frametitle{Indian Buffet Process (IBP)}
    Customer $i$ taking a dish $k$ is analogous to observation $i$ possessing
    feautre $k$. For $\bm Z \sim \text{IBP}_N(\alpha)$:\\
    \wl
    \begin{enumerate}
      \setlength\itemsep{1em}
      \item The first customer takes Poisson($\alpha$) number of dishes
      \item For customers $i=2 \text{ to } N$,
      \begin{itemize}
        \item For each previously sampled dish,
              customer $i$ takes dish $k$ with probability $m_k / i$
        \item after sampling the last sampled dish, customer $i$ samples 
              Poisson($\alpha/i$) new dishes
      \end{itemize}
    \end{enumerate}
  }

  \frame{
    \frametitle{PMF for the IBP}
      \[
      \begin{array}{rcl}
        \underset{N \times \infty}{\bm Z} &\sim& \text{IBP}(\alpha),
           \text{ where $z_{ik} \in \{0,1\}$} \\
        \implies \text{P}(\bm Z) & = & \frac{\alpha^{K}}{\prodl{i}{1}{N} 
                                       x_i!} 
                                       exp\{-\alpha H_N\}\prodl{k}{1}{K}
                                       \frac{(N-m_k)!(m_k-1)!}{N!}\\
                                 & = & \frac{\alpha^{K}exp\{-\alpha H_N\}}
                                            {\prodl{i}{1}{N}(x_i!~i^{x_i})} 
                                       \prodl{i}{2}{N}\prodl{k}{1}{y_{i}}
                                       \left(\frac{m_{-i,k}}{i}\right)^{z_{i,k}}
                                       \left(1-\frac{m_{-i,k}}{i}\right)^{1-z_{i,k}}
      \end{array}
      \]
      where $H_N$ is the harmonic number $\suml{i}{1}{N}i^{-1}$, $K$ is the
      number of non-zero columns in $\bm Z$, $m_k$ is the $k^{th}$ column sum
      of $\bm Z$, $y_i$ is the total number of dishes sampled before the
      $i^{th}$ customer, $m_{-i,k}$ is the number of customers that sampled
      dish k before customer $i$,and $x_i$ is the number of ``new dishes"
      sampled by customer $i$.\\
  }

  \frame{
    \frametitle{Realizations from the IBP}
    \beginmyfig
<<fig=T,echo=F,height=5>>=
    set.seed(2)
    par(mfrow=c(1,2))
      Z1 <- raibp(N=5,a=2)
      a.image(Z1,main="Figure 1: IBP(N=50, a=2)",cex.main=.8,axis.num=F,
              color=c("gray100","gray30"))
      axis(2,at=seq(1,0,len=nrow(Z1)),label=paste("Cust",1:nrow(Z1)),las=2)
      axis(1,at=seq(0,1,len=ncol(Z1)),label=paste("Col",1:ncol(Z1)),las=2)
      Z2 <- raibp(N=5,a=5)
      a.image(Z2,main="Figure 2: IBP(N=50, a=5)",cex.main=.8,axis.num=F,
              col=c("gray100","gray30"))
      axis(2,at=seq(1,0,len=nrow(Z2)),label=paste("Cust",1:nrow(Z2)),las=2)
      axis(1,at=seq(0,1,len=ncol(Z2)),label=paste("Col",1:ncol(Z2)),las=2)
    par(mfrow=c(1,1))
@
    \endmyfig
  }

\section{AIBP}  
  \frame{
    \frametitle{Attraction Indian Buffet Process (AIBP)}
    For $\bm Z \sim \text{AIBP}(\alpha,\bm D)$\\
    \wl
    \begin{enumerate}
      \item The first customer takes Poisson($\alpha$) number of dishes
      \item For customers $i=2 \text{ to } N$,
      \begin{itemize}
        \item For each previously sampled dish,
              customer $i$ takes dish $k$ with probability $p_{i,k}$
        \item after sampling the last sampled dish, customer $i$ samples 
              Poisson($\alpha/i$) new dishes
      \end{itemize}
    \end{enumerate}
    \wl
    where $p_{i,k} = \hik\frac{m_{-i}}{i}$.
  }
  
  \frame{
    \frametitle{PMF for the AIBP}
    \begin{multline*}
      P(\bm{Z|D},\alpha) = 
        \ds\frac{\alpha^K \exp(-\alpha H_N)} {\prodl{i}{1}{N}(x_i!~i^{x_i})} 
        \prodl{i}{2}{N}\prodl{k}{1}{y_i} p_{i,k}^{z_{i,k}} (1-p_{i,k})^{1-z_{i,k}}
      %\ds\frac{\alpha^K \exp(-\alpha H_N)} {\prodl{i}{1}{N}x_i!} 
      %\prodl{i}{2}{N} i^{-(x_i+y_{-i})} \times 
      %\\
      %\prodl{k}{1}{y_{-i}} 
      %\ds\frac{
      %%\left(\sums{\sigma}{z_{-i,k}=1}{}\lambda(\sigma,\sigma_i)m_{-i}\right)^{z_{i,k}}
      %%\left(i-\sums{\sigma}{z_{-i,k}=1}{}
      %%\lambda(\sigma,\sigma_i)m_{-i}\right)^{1-z_{i,k}}
      %\left(\suml{j}{1}{i-1}\lambda(\sigma_j,\sigma_i)z_{j,k}m_{-i}\right)^{z_{i,k}}
      %\left(i-\suml{j}{1}{i-1}
      %\lambda(\sigma_j,\sigma_i)z_{j,k}m_{-i}\right)^{1-z_{i,k}}
      %}
      %%{\suml{k}{1}{y_{-i}}\sums{\sigma}{z_{-i,k}=1}{}\lambda(\sigma,\sigma_i)}\right)
      %{\suml{k}{1}{y_{-i}}\suml{j}{1}{i-1}\lambda(\sigma_j,\sigma_i)z_{j,k}}
    \end{multline*}
    where $\lambda(i,j) = f(d_{ij})$ and $f(.)$ is a decay function that
    measures the proximity between customers $i$ and $j$, and $m_{-i}$ is the
    total number of dishes taken by all customers before customer $i$.
  }

  \frame{
    \frametitle{PMF for the IBP \& AIBP}
      \[
      \begin{array}{rcl}
        \text{IBP:} &&\\
        \text{P}(\bm Z) & = & \frac{\alpha^{K}exp\{-\alpha H_N\}}
                                   {\prodl{i}{1}{N}(x_i!~i^{x_i})} 
                              \prodl{i}{2}{N}\prodl{k}{1}{y_{i}}
                              \left(\frac{m_{-i,k}}{i}\right)^{z_{i,k}}
                              \left(1-\frac{m_{-i,k}}{i}\right)^{1-z_{i,k}}\\
        &&\\
        \text{AIBP:}&&\\
        \text{P}(\bm{Z|D},\alpha) & = & 
                              \frac{\alpha^K \exp(-\alpha H_N)} 
                                   {\prodl{i}{1}{N}(x_i!~i^{x_i})} 
                              \prodl{i}{2}{N}\prodl{k}{1}{y_i} 
                              p_{i,k}^{z_{i,k}} (1-p_{i,k})^{1-z_{i,k}}
      \end{array}  
      \]
  }

  \frame{
    \frametitle{Realizations from the AIBP}
    \beginmyfig
<<fig=T,echo=F,height=5>>=
    set.seed(2)
    DD <- matrix(c(0,9,1,9,9,
                   9,0,9,9,1,
                   1,9,0,9,9,
                   9,9,9,0,9,
                   9,1,9,9,0),5,5)
    par(mfrow=c(1,3))
      a.image(DD,axis.num=F,numbers=T,main="Distance Matrix (D)")
      Z1 <- raibp(N=nrow(DD),D=DD,a=2)
      a.image(Z1,main="Figure 1: AIBP(N=5, a=2, D)",cex.main=.8,axis.num=F,
              color=c("gray100","gray30"))
      axis(2,at=seq(1,0,len=nrow(DD)),label=paste("Cust",1:nrow(DD)),las=2)
      axis(1,at=seq(0,1,len=ncol(Z1)),label=paste("Col",1:ncol(Z1)),las=2)
      Z2 <- raibp(N=nrow(DD),D=DD,a=5)
      a.image(Z2,main="Figure 1: AIBP(N=5, a=5, D)",cex.main=.8,axis.num=F,
              color=c("gray100","gray30"))
      axis(2,at=seq(1,0,len=nrow(DD)),label=paste("Cust",1:nrow(DD)),las=2)
      axis(1,at=seq(0,1,len=ncol(Z2)),label=paste("Col",1:ncol(Z2)),las=2)
    par(mfrow=c(1,1))
@
    \endmyfig 
  }
   
\section{Comparisons}
  \frame{
    \frametitle{Distance Matrix \& Decay Function}
    \begin{table}[ht]
    \centering
    \[
      \begin{pmatrix}{}
        0 & 9 & 1 & 9 & 9 \\ 
        9 & 0 & 9 & 9 & 1 \\ 
        1 & 9 & 0 & 9 & 9 \\ 
        9 & 9 & 9 & 0 & 9 \\ 
        9 & 1 & 9 & 9 & 0 \\ 
      \end{pmatrix}
    \]
    \caption{Distance Matrix used in this simulation study. Notice that
             all customers are equidistant from each other except for 
             customers 1 \& 3 and 2 \& 5, who are much closer together.}
    \end{table}

    The decay function used for this simulation study is the exponential decay
    function $f(d) = exp(-d)$. Notice that $f(0)=1 \text{ and } f(\infty)=1$.
  }

  \frame{
    \frametitle{Comparisons of Expected Values for Equidistant Observations}
    \begin{figure}\begin{center}
      \includegraphics[height=.7\textwidth]{raw/uni.pdf}
      %\caption{The expected values of draws from the IBP were computed via
      %         simulation.  Ten thousand realizations were drawn from the IBP
      %         with $\alpha=.5$.  The ten thousand matrices were then summed
      %         across and divided by the number of realizations (10,000).
      %         Because draws from the IBP do not always have the same
      %         dimensions, each of the matrices were first padded with zeros to
      %         make matrix summation conformable. Expected values were computed
      %         in a similar way for the AIBP and ddIBP. In simulation, we have
      %         shown that the AIBP and ddIBP reduce to the IBP when the
      %         similarity function is set to a constant value. E[ncol] is the
      %         expected number of columns for a given distribution. Values in
      %         the cells of the matrices are the probabilities of those cells
      %         taking the value `1' as opposed to 0. The colors are an
      %         additional indication of the probabilities, red is used for
      %         cells with higher probability; white is used for cells with
      %         lower probability. The numbers to the left of the grids are the
      %         expected row sums. The numbers at the bottom of the grids are
      %         the expected column sums.}
    \end{center}\end{figure}
  }

  \frame{
    \frametitle{Expectations given a Distance Matrix}
    \begin{figure}\begin{center}
      \includegraphics[height=.7\textwidth]{raw/eSim.pdf}
      %\caption{NEED A CAPTION!}
    \end{center}\end{figure}
  }

  \frame{
    \frametitle{Expectations given a Distance Matrix and First Two Rows}
    \begin{figure}\begin{center}
      \includegraphics[height=.7\textwidth]{raw/eSimFixed.pdf}
      %\caption{NEED A CAPTION!}
    \end{center}\end{figure}
  }

  \frame{
    \frametitle{Properties of the AIBP}
    \begin{center}
      \resizebox{.8\textwidth}{!}{%
      \begin{tabular}{c|c|c}
        \hline
          Comparison & AIBP & ddIBP \\ \hline \hline
          Explicit pmf & Yes & No \\ \hline
          Expected non-zero columns equal to that of IBP & Yes & No \\ \hline
          Expected row sums equal to that of IBP & Yes & Yes \\ \hline
          Expected column sums equal to that of IBP & No & No \\ \hline
          Expected matrix sum equal to that of IBP & Yes & Yes \\ \hline
        \hline
      \end{tabular}%
      }
      \captionof{table} {Comparisons of the AIBP to the ddIBP showing how what 
                         properties of the IBP they presrve.}
    \end{center}
  }

\end{document}
